import.packages("mlbench")
install.packages("mlbench")
install.packages("caret")
library(mlbench)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame
## One reason is that this will five the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector 'y' and a matrix
## of predictors 'x'.  Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
library(caret)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProcess = c("center", "scale"),
tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = knnPred, obs = testData$y)
install.packages("plotmo")
library(mlbench)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame
## One reason is that this will five the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector 'y' and a matrix
## of predictors 'x'.  Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
library(caret)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProcess = c("center", "scale"),
tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = knnPred, obs = testData$y)
library(plotmo)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:15)
# Fix the seed tso that the results can be reproduced
set.seed(123)
marsTuned <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = MARS_grid,
preProcess = c("center", "scale"),
tuneLength = 10)
library(plotmo)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:15)
# Fix the seed tso that the results can be reproduced
set.seed(123)
marsTuned <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = MARS_grid,
preProcess = c("center", "scale"),
tuneLength = 10)
library(plotmo)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:15)
# Fix the seed tso that the results can be reproduced
set.seed(123)
marsTuned <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProcess = c("center", "scale"),
tuneLength = 10)
marsTuned
marsTuned$bestTune
ggplot(marsTuned)
marsTuned$finalModel
varImp(marsTuned)
plot(varImp(marsTuned))
plotmo(marsTuned)
install.packages("nnet")
install.packages("nnet")
install.packages("nnet")
install.packages("neural")
install.packages("RSNNS")
## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(1:10), bag = FALSE)
set.seed(123)
nnetTuned <- train(trainingData$x, trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = trainControl(method = "cv", number = 10),
preProc = c("center", "scale"),
linout = TRUE,  trace = FALSE,
MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
maxit = 500)
library(nnet)
## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(1:10), bag = FALSE)
set.seed(123)
nnetTuned <- train(trainingData$x, trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = trainControl(method = "cv", number = 10),
preProc = c("center", "scale"),
linout = TRUE,  trace = FALSE,
MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
maxit = 500)
library(nnet)
library(neural)
## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(1:10), bag = FALSE)
set.seed(123)
nnetTuned <- train(trainingData$x, trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = trainControl(method = "cv", number = 10),
preProc = c("center", "scale"),
linout = TRUE,  trace = FALSE,
MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
maxit = 500)
library(nnet)
library(neural)
library(RSNNS)
## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(1:10), bag = FALSE)
set.seed(123)
nnetTuned <- train(trainingData$x, trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = trainControl(method = "cv", number = 10),
preProc = c("center", "scale"),
linout = TRUE,  trace = FALSE,
MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
maxit = 500)
library("tidyverse")
install.packages("tidyverse")
install.packages("forecast")
library("tidyverse")
library("forecast")
library(nnet)
library(neural)
library(RSNNS)
## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(1:10), bag = FALSE)
set.seed(123)
nnetTuned <- train(trainingData$x, trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = trainControl(method = "cv", number = 10),
preProc = c("center", "scale"),
linout = TRUE,  trace = FALSE,
MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
maxit = 500)
install.packages("tidyverse", "caret", "plotmo", "earth", "kernlab", "forecast", "ipred", "mlbench", "AppliedPredictiveModeling")
install.packages("tidyverse", "caret", "plotmo", "earth", "kernlab", "forecast", "ipred", "mlbench", "AppliedPredictiveModeling")
install.packages("tidyverse", "caret", "plotmo", "earth", "kernlab", "forecast", "ipred", "mlbench", "AppliedPredictiveModeling")
install.packages("tidyverse", "caret", "plotmo", "earth", "kernlab", "forecast", "ipred", "mlbench", "AppliedPredictiveModeling")
install.packages("kernlab", "forecast", "ipred", "mlbench", "AppliedPredictiveModeling")
install.packages("kernlab", "forecast", "ipred", "mlbench", "AppliedPredictiveModeling")
install.packages("forecast", "ipred", "mlbench", "AppliedPredictiveModeling")
install.packages("forecast", "ipred", "mlbench", "AppliedPredictiveModeling")
install.packages("AppliedPredictiveModeling")
library("tidyverse")
library("forecast")
library("AppliedPredictiveModeling")
library(nnet)
library(neural)
library(RSNNS)
## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(1:10), bag = FALSE)
set.seed(123)
nnetTuned <- train(trainingData$x, trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = trainControl(method = "cv", number = 10),
preProc = c("center", "scale"),
linout = TRUE,  trace = FALSE,
MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
maxit = 500)
install.packages("e1071")
library("tidyverse")
library("e1071")
library("forecast")
library("AppliedPredictiveModeling")
library(nnet)
library(neural)
library(RSNNS)
## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(1:10), bag = FALSE)
set.seed(123)
nnetTuned <- train(trainingData$x, trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = trainControl(method = "cv", number = 10),
preProc = c("center", "scale"),
linout = TRUE,  trace = FALSE,
MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
maxit = 500)
library(caret)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProcess = c("center", "scale"),
tuneLength = 10)
knnModel
library(nnet)
library(neural)
library(RSNNS)
## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(1:10), bag = FALSE)
set.seed(123)
nnetTuned <- train(trainingData$x, trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = trainControl(method = "cv", number = 10),
preProc = c("center", "scale"),
linout = TRUE,  trace = FALSE,
MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
maxit = 500)
nnetTuned
nnetTuned$bestTune
nnetTuned$finalModel
ggplot(nnetTuned)
varImp(nnetTuned)
varImp(nnetTuned)
plot(varImp(nnetTuned))
set.seed(123)
svmTuned <- train(trainingData$x, trainingData$y,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = trainControl(method = "cv"))
svmTuned
svmTuned$bestTune
svmTuned$finalModel
ggplot(svmTuned)
varImp(svmTuned)
plot(varImp(svmTuned))
marsPred <- predict(marsTuned, newdata = testData$x)
marspv <- postResample(pred = marsPred, obs = testData$y) #performance values
nnPred <- predict(nnetTuned, newdata = testData$x)
nnpv <- postResample(pred = nnPred, obs = testData$y)
svmPred <- predict(svmTuned, newdata = testData$x)
svmpv <- postResample(pred = svmPred, obs = testData$y)
knnPred <- predict(knnTune, newdata = testData$x)
marsPred <- predict(marsTuned, newdata = testData$x)
marspv <- postResample(pred = marsPred, obs = testData$y) #performance values
nnPred <- predict(nnetTuned, newdata = testData$x)
nnpv <- postResample(pred = nnPred, obs = testData$y)
svmPred <- predict(svmTuned, newdata = testData$x)
svmpv <- postResample(pred = svmPred, obs = testData$y)
knnPred <- predict(knnModel, newdata = testData$x)
knnpv <- postResample(pred = knnPred, obs = testData$y)
marsPred <- predict(marsTuned, newdata = testData$x)
marspv <- postResample(pred = marsPred, obs = testData$y)
marspv
nnPred <- predict(nnetTuned, newdata = testData$x)
nnpv <- postResample(pred = nnPred, obs = testData$y)
nnpv
svmPred <- predict(svmTuned, newdata = testData$x)
svmpv <- postResample(pred = svmPred, obs = testData$y)
svmpv
knnPred <- predict(knnModel, newdata = testData$x)
knnpv <- postResample(pred = knnPred, obs = testData$y)
knnpv
knnModel$bestTune
ggplot(knnModel)
knnModel$finalModel
varImp(knnModel)
plot(varImp(knnModel))
marsPred <- predict(marsTuned, newdata = testData$x)
marsPv <- postResample(pred = marsPred, obs = testData$y)
marsPv
nnPred <- predict(nnetTuned, newdata = testData$x)
nnPv <- postResample(pred = nnPred, obs = testData$y)
nnPv
svmPred <- predict(svmTuned, newdata = testData$x)
svmPv <- postResample(pred = svmPred, obs = testData$y)
svmPv
knnPred <- predict(knnModel, newdata = testData$x)
knnPv <- postResample(pred = knnPred, obs = testData$y)
knnPv
data("ChemicalManufacturingProcess")
preprocessing <- preProcess(ChemicalManufacturingProcess[,-1], method = c("center", "scale", "knnImpute", "corr", "nzv"))
Xpreprocess <- predict(preprocessing, ChemicalManufacturingProcess[,-1])
yield <- as.matrix(ChemicalManufacturingProcess$Yield)
set.seed(789)
split2 <- yield %>%
createDataPartition(p = 0.8, list = FALSE, times = 1)
Xtrain.data  <- Xpreprocess[split2, ]
xtest.data <- Xpreprocess[-split2, ]
Ytrain.data  <- yield[split2, ]
ytest.data <- yield[-split2, ]
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(1:10), bag = FALSE)
set.seed(234)
chemNnetTuned <- train(Xtrain.data, Ytrain.data,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = trainControl(method = "cv", number = 10),
linout = TRUE,  trace = FALSE,
MaxNWts = 10 * (ncol(Xtrain.data) + 1) + 10 + 1,
maxit = 500)
install.packages("RANN")
install.packages("RANN")
library(RANN)
data("ChemicalManufacturingProcess")
preprocessing <- preProcess(ChemicalManufacturingProcess[,-1],
method = c("center", "scale", "knnImpute", "corr", "nzv"))
preprocess <- predict(preprocessing, ChemicalManufacturingProcess[,-1])
yield <- as.matrix(ChemicalManufacturingProcess$Yield)
set.seed(123)
split2 <- createDataPartition(yield, p = 0.8, list = FALSE, times = 1)
Xtrain.data2  <- preprocess[split2, ]
xtest.data2 <- preprocess[-split2, ]
Ytrain.data2  <- yield[split2, ]
ytest.data2 <- yield[-split2, ]
ridge.grid <- data.frame(.lambda = seq(0, .1, length = 15))
set.seed(123)
ridge.mod2 <- train(x=Xtrain.data2,
y=Ytrain.data2,
method="ridge",
tuneGrid=ridge.grid,
trControl= ctrl)
library(RANN)
data("ChemicalManufacturingProcess")
preprocessing <- preProcess(ChemicalManufacturingProcess[,-1],
method = c("center", "scale", "knnImpute", "corr", "nzv"))
preprocess <- predict(preprocessing, ChemicalManufacturingProcess[,-1])
yield <- as.matrix(ChemicalManufacturingProcess$Yield)
set.seed(123)
split2 <- createDataPartition(yield, p = 0.8, list = FALSE, times = 1)
Xtrain.data2  <- preprocess[split2, ]
xtest.data2 <- preprocess[-split2, ]
Ytrain.data2  <- yield[split2, ]
ytest.data2 <- yield[-split2, ]
ridge.grid <- data.frame(.lambda = seq(0, .1, length = 15))
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10)
ridge.mod2 <- train(x=Xtrain.data2,
y=Ytrain.data2,
method="ridge",
tuneGrid=ridge.grid,
trControl= ctrl)
ridge.mod2
plot(ridge.mod2)
ridge.mod2$bestTune
ridge.mod2$finalModel
ggplot(ridge.mod2)
varImp(ridge.mod2)
plot(varImp(ridge.mod2))
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(1:10), bag = FALSE)
set.seed(123)
chemNnetTuned <- train(Xtrain.data2, Ytrain.data2,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = trainControl(method = "cv", number = 10),
linout = TRUE,  trace = FALSE,
MaxNWts = 10 * (ncol(Xtrain.data) + 1) + 10 + 1,
maxit = 500)
marsGrid <- expand.grid(.degree = 1:3, .nprune = 2:100)
set.seed(123)
chemMarsTuned <- train(Xtrain.data2, Ytrain.data2,
method = "earth",
tuneGrid = marsGrid,
trControl = trainControl(method = "cv", number = 10))
set.seed(123)
chemSvmTuned <- train(Xtrain.data2, Ytrain.data2,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = trainControl(method = "cv"))
set.seed(123)
chemKnnTuned <- train(Xtrain.data2,
Ytrain.data2,
method = "knn",
tuneGrid = data.frame(.k = 1:20),
trControl = trainControl(method = "cv"))
chemKnnTuned$bestTune
chemKnnTuned$finalModel
ggplot(chemKnnTuned)
varImp(chemKnnTuned)
plot(varImp(chemKnnTuned))
marsPred2 <- predict(chemMarsTuned, newdata = xtest.data2)
marsPv2 <- postResample(pred = marsPred2, obs = ytest.data2)
marsPv2
nnPred2 <- predict(chemNnetTuned, newdata = xtest.data2)
nnPv2 <- postResample(pred = nnPred2, obs = ytest.data2)
nnPv2
svmPred2 <- predict(chemSvmTuned, newdata = xtest.data2)
svmPv2 <- postResample(pred = svmPred2, obs = ytest.data2)
svmPv2
knnPred2 <- predict(chemKnnModel, newdata = xtest.data2)
marsPred2 <- predict(chemMarsTuned, newdata = xtest.data2)
marsPv2 <- postResample(pred = marsPred2, obs = ytest.data2)
marsPv2
nnPred2 <- predict(chemNnetTuned, newdata = xtest.data2)
nnPv2 <- postResample(pred = nnPred2, obs = ytest.data2)
nnPv2
svmPred2 <- predict(chemSvmTuned, newdata = xtest.data2)
svmPv2 <- postResample(pred = svmPred2, obs = ytest.data2)
svmPv2
knnPred2 <- predict(chemKnnTuned, newdata = xtest.data2)
knnPv2 <- postResample(pred = knnPred2, obs = ytest.data2)
knnPv2
ridgePred <- predict(ridge.mod2, newdata = xtest.data2)
ridgePv2 <- postResample(pred = ridgePred, obs = ytest.data2)
ridgePv2
predictions <- predict(ridge.mod2, xtest.data2)
marsPred2 <- predict(chemMarsTuned, newdata = xtest.data2)
marsPv2 <- postResample(pred = marsPred2, obs = ytest.data2)
marsPv2
nnPred2 <- predict(chemNnetTuned, newdata = xtest.data2)
nnPv2 <- postResample(pred = nnPred2, obs = ytest.data2)
nnPv2
svmPred2 <- predict(chemSvmTuned, newdata = xtest.data2)
svmPv2 <- postResample(pred = svmPred2, obs = ytest.data2)
svmPv2
knnPred2 <- predict(chemKnnTuned, newdata = xtest.data2)
knnPv2 <- postResample(pred = knnPred2, obs = ytest.data2)
knnPv2
varImp(chemMarsTuned)
varImp(ridge.mod2)
varImp(chemSvmTuned)
plot(varImp(chemSvmTuned))
varImp(chemSvmTuned)
varImp(ridge.mod2)
cor(yield, ChemicalManufacturingProcess$ManufacturingProcess32)
cor(yield, ChemicalManufacturingProcess$ManufacturingProcess13)
library(tidyverse)
library(e1071)
library(forecast)
library(AppliedPredictiveModeling)
library(caret)
library(mlbench)
library(nnet)
library(neural)
library(RSNNS)
library(plotmo)
require(caret)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProcess = c("center", "scale"),
tuneLength = 10)
knnModel
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(1:10), bag = FALSE)
set.seed(123)
chemNnetTuned <- train(Xtrain.data2, Ytrain.data2,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = trainControl(method = "cv", number = 10),
linout = TRUE,  trace = FALSE,
MaxNWts = 10 * (ncol(Xtrain.data) + 1) + 10 + 1,
maxit = 500)
chemNnetTuned
